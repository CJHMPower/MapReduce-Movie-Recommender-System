{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MapReduce Moive Recommender System\n",
    "\n",
    "## Overview\n",
    "\n",
    "This is a movie Recommender System project based on [Item Collaborative Filtering](https://en.wikipedia.org/wiki/Item-item_collaborative_filtering). It is run on Hadoop MapReduce Cluster environment. This project is still in progress. More experiments and results will be released in this resporitory further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Preprocess\n",
    "\n",
    "The dataset used for this movie Recommender System comes from the [Netflix Prize](https://www.netflixprize.com/). The original Netflix dataset consists of movie ratings from 480189 users and 17770 movies, which is too large to run on Pseudo-Distrubuted Mode. As a result, a parameter ** n ** can be specfied to select the first **n** movies from the original training set. 20% of the training data will be split for testing set (See the ```data_preprocess.py``` file for more details). All the selected data will be written into a single txt file as the raw input data for MapReduce job. The input format for both train and test data is:\n",
    "\n",
    "<code>   ** userID,movieId,rating **\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms\n",
    "\n",
    "The Item Collaborative Filtering Algorithms can be divided into two main steps\n",
    "\n",
    "### 1.Compute the similarity between items\n",
    "\n",
    "The most commonly used metric for items is Cosine Similarity\n",
    "\n",
    "<center> $$ W_{ij} = \\frac{|N(i)| \\bigcap |N(j)|}{\\sqrt{|N(i)||N(j)|}} $$ </center>\n",
    "\n",
    "in which $ |N(i)| $ is the number of users interested in item ```i```, $ |N(j)| $ is the number of users interested in item ```j```. $|N(i)| \\bigcap |N(j)|$ is the number of user interested in both item ```i``` and ```j```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Compute prediction ratings for users' rating history\n",
    "\n",
    "After acquiring the similarities between each pair of movies, we can make predictions by users' rating history. The rating of user ```u``` to item ```j``` is calculated as below\n",
    "\n",
    "<center> $$ P_{uj} = \\frac{\\sum_{i \\in N(u)} w_{ji}r_{ui}}{\\sum_{i\\in N(u)}w_{ji}}  $$\n",
    "\n",
    "in which $ N(u)$ is the set of items rated by user ```u```. $w_{ji}$  is the similarity between item ```i``` and ```j```, $r_{ui}$  is the previous rating of item ```i``` from user ```u```\n",
    "\n",
    "This Recommender System was finally evaluated on test dataset by the Root Mean Square Error\n",
    "<center> $$ RMSE = \\sqrt{ \\frac{1}{n} \\sum_{uj} (P_{uj} - R_{uj})^2} $$\n",
    "\n",
    "in which, $ R_{uj}$ is the ground truth rating of user ```u``` to movie ```j```, $ P_{uj}$ is the prediction made by recommender sytem, ```n``` is the total number of test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are total 8 MapReduce jobs used for implementing this algorithm in Hadoop distributed file system. \n",
    "\n",
    "* DataDivideByUser.java: Split raw-input data by userId. \n",
    "\n",
    "* coOccurrenceMatrix.java: Build co-occurence matrix for each movies.\n",
    "\n",
    "* Normalize.java and Normalize2.java: These two mapreduce jobs were used to normalize the similarities. In each of them, the co-occurence matrix was divided by $ \\sqrt{|N(i)|}$ and $ \\sqrt{|N(j)|}$ respectively.\n",
    "\n",
    "* Multiplication.java: Use mapreduce matrix multiplication to multiply co-occurrce matrix and rating matrix.\n",
    "\n",
    "* Sum.java: Generating rating predictions for each pair of user and movie.\n",
    "\n",
    "* RMSE.java: This mapreduce job will compute square error for each prediction on test dataset.\n",
    "\n",
    "* Result.java: Collect all square errors and output the Root Mean Square Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "* Ubuntu 16.04\n",
    "* Python 2.7\n",
    "* Java 1.8.1\n",
    "* Hadoop "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Run\n",
    "* Clone this respirtory and download the Netflix dataset into the ```data``` folder. \n",
    "\n",
    "* Start hadoop and enter this directory\n",
    "\n",
    "* Run python ```data_preprocess.py -n 500 ``` to generate raw-input data for mapreduce.\n",
    "You can choose the number of movies used by your preference and machine.\n",
    "\n",
    "* Run ./run_hadoop.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result and Performance\n",
    "\n",
    "The "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
